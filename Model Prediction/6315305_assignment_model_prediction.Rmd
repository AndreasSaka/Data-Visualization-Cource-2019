---
title: "6315305_assignment_prediction_model"
author: "Andreas Sakapetis"
date: "January 18, 2019"
output:
  pdf_document: default
  html_document: default
---

__Loading neccessary libraries__
```{r}
#loading the library "readxl" to be able to use the function read_xlsx().
library(readxl)
#loading the library "formattable" to introduce a table with all the variables.
library(formattable)
#loading the library "ggplot2" to create usefull plots.
library(ggplot2)
#loading the library "GGally" to create usefull plots.
library(GGally)
#loading the library "tidyverse" to perform varius manipulations.
library(tidyverse)
#loading the library "car" to perform VIF.
library(car)
#loading the library "leaps" to perform subset selection.
```

__Import the data from local folder to Rstudio in p.student variable.__
__The dataset can be found from kaggle(https://www.kaggle.com/spscientist/students-performance-in-exams/version/1)__
```{r}
p.student <- 
  read.csv("C:\\Users\\Andreas\\Desktop\\Assignment B\\StudentsPerformance.csv",
           header = TRUE)

```

__View the names of the columns in the p.student dataset.__
```{r}
names(p.student)
```
__Use the head() function to examine at the first few rows of the p.student dataset.__
```{r}
head(p.student)

```
__Use the function tail() to examine the last few rows of the p.student dataset.__
```{r}
tail(p.student)
```
This data set includes scores from three exams and a variety of personal, social,
and economic factors that have interaction effects upon them.The dataset consists
of 8 variables, with a total of 1000 observations. There are no missing values.
Within the dataset exist variables that with the type being either an interger 
or a factor.

In the study sample, 482/1000 (48.2%) of the students are male 518/1000 (51.8%)
are female. The mean math.score is 66.089 points with a standard deviation of
15.16 points (range 0-100 points). The mean of reading.score is 69.169 points
with a standard deviation of 14 points (range 0-100 points). 

Approximately 31.9% of the sudents are enthnicity group C, 26.2% are group D, 
19% are group B, 14% are group E and 8.9% group A. 64.2% of the students did none preparation
and 35.8% did a complete preparation. 


64.5 % of the students had a standard lunch, while 35.5% of the students had free 
or reduced lunch. 22.6% of the stundets has parents that acquired some college
education, 22.2% associate's degree education, 19.6% of the students have parents 
with high scool education and 17.9% some high scool education. 11.8% of the students
have parents with a bachelor's degree and the most scarce of all 5.9% of the students
have parents with a master's degree.



__This function counts missing values__
```{r}
sapply(p.student, function(x) sum(length(which(is.na(x)))))
```
__With sapply function it is possible to examine the type of the variables.__
```{r}
sapply(p.student,class)
```
__With the summary fucntion, the summary statistics for the p.student dataset are obtained.__
```{r}
summary(p.student)
```   

__These plots are assisting in identifying trends in the data, using as response__
__variable "Math score".__
```{r}
plot(p.student$math.score~.,data=p.student)
```

__These plots are in identifying trends in the data, using as response__
__variable "Reading score".__
```{r}
plot(p.student$reading.score~.,data=p.student)
```

__These plots arein identifying trends in the data, using as response__
__variable "Writing score".__
```{r}
plot(p.student$writing.score~.,data=p.student)
```
__From the scatterplots we can see that the variables reading.score, writing.score__
__and math.score appear to have a linear relationship.__



```{r}
ggplot(p.student)+
geom_histogram(mapping = aes(x = p.student$math.score))+
geom_vline(xintercept=mean(p.student$math.score), lwd=1, linetype=2, color="black")+
  ggtitle("Math Score Distribution")
  
```
```{r}
ggplot(p.student)+
geom_histogram(mapping = aes(x = p.student$reading.score))+
geom_vline(xintercept=mean(p.student$reading.score), lwd=1, linetype=2, color="black")+
  ggtitle("Reading Score Distribution") 
```
```{r}
ggplot(p.student)+
geom_histogram(mapping = aes(x = p.student$writing.score))+
geom_vline(xintercept=mean(p.student$writing.score), lwd=1, linetype=2, color="black")+
  ggtitle("Writing Score Distribution") 
```
```{r}
p.student %>% 
  ggplot(aes(x = test.preparation.course , y = math.score, fill = p.student$gender)) +
  geom_boxplot() +
  theme_minimal()+
  ggtitle("Math Score BoxPlot")

```
__From the boxplot examination females seem to do less better than males on math.__
__From the plot completed preparation is concentrating better scores in math__
__than none preperation.__
__Outliers are spotted even though from visual examination of the dataset only a__
__single value was "0", which can be interpreated as poor performace.__
__The outliers are more spread for females and more concentrated for males.__

```{r}
p.student %>% 
  ggplot(aes(x = parental.level.of.education , y = reading.score, fill = p.student$gender)) +
  geom_boxplot() +
  theme_minimal()+
  ggtitle("Reading Score BoxPlot")
```
```{r}
p.student %>% 
  ggplot(aes(x = race.ethnicity , y = writing.score, fill = p.student$gender)) +
  geom_boxplot() +
  theme_minimal()+
  ggtitle("Writing Score BoxPlot")
```

__Preparing the dataset to execute the multiple linear regression.__
```{r}
#Splitting the dataset into a training(75%) and testing set(25%).

#Create a variable "split" that contains 750 times the word train and 250 the 
#word test.
split <- c(rep("train", 750), rep("test",  250))

#Divide the dataset based on the words train and test.
p.student <- p.student %>% mutate(split = sample(split))

#Delete the column split
p.student_train <- p.student %>% filter(split == "train")
p.student_train$split <-  NULL

#Delete the column split
p.student_test <- p.student %>% filter(split == "test")
p.student_test$split <-  NULL

```

__Executing the multiple linear regression.__


```{r}
#The multiple regression model is selected since the input variable is quantitative.
model1 <- lm(formula=p.student_train$math.score ~ . , data=p.student_train)
#Aquire the summary statistics of the multiple linear regression.
summary(model1)

```
__As it is possible to examine from the summary(), regression coefficient gender__
__for male is associated with an increase of 13 points on math.score against females. __
__The regression coefficient Race ethnicity E has also significant association __
__with an increase of 5 points on math.score against Race ethnicity A.__

__However,the regression coefficient race.ethnicities B,C and D do not reach statistical__
__significance. Same applies for the regression coefficient parental.level.of.education__
__for the education levels of: some high school,some college,master's degree __
__high school. On the contrast regression coefficient parental.level.of.education __
__bachelor's degree is associated with a decrease of 1.7 points on math.score __
__against asscociate's degree.__

__The regression coefficient Lunch is associated with an increase of 3.3 points__
__on math.score against free/reduced. The regression coefficient Reading.score__
__is associated with an increase of 0.22 points on math.score. The regression__
__coefficient writing.score is associated with an increas of 0.71 on math.score.__

__The regression coefficient test.preparation none is associated with an increase__
__of 3.4 against completed preparation.__

__The multiple regression estimates $\beta_0$ (the intercept) and__ __$\beta_1$,$\beta_2$,$\beta_3$,$\beta_4$,$\beta_5$,$\beta_6$ and $\beta_7$__
__in the following equation:__

$$\boldsymbol{y} = \beta_0 + \beta_1\cdot \boldsymbol{x}_1 + \beta_2\cdot \boldsymbol{x}_2+ \beta_3\cdot \boldsymbol{x}_3+\beta_4\cdot \boldsymbol{x}_4+\beta_5\cdot \boldsymbol{x}_5+\beta_6\cdot \boldsymbol{x}_6+ \beta_7\cdot \boldsymbol{x}_7+\boldsymbol{\epsilon}$$


__Through the examination of the multiple regression analysis. The Fstatistic__
__p-value is < 2.2e-16. This leads to the conclusion that, there is at least, __
__one predictor variable which is significantly related to the outcome variable.__

__The adjusted R-squared indicates that 87% of the variation in math score can be __
__explained by the model containing gender,race.ethnicity,parental.level.of.education,__
__lunch, test.preparation, reading.score and writing score.__ 

__To examine which predictor variables are significant, we require estimate of__
__regression beta coefficients and the associated t-statitic p-values:__

```{r}
summary(model1)$coefficient
```
__The variables that were selected for the model based on p-value are: gender, lunch,__ __test.preparation.course,reading.score and writing.score__



__Before proceding with further investigation, VIF will be examined to find__
__whether or not there is colinearity between the predictors.__
```{r}
vif(model1)


```
__The mean VIF is well under 10, thus there is not multicollinearity .__
```{r}
reduced <- 
  lm(formula=p.student_train$math.score ~gender+lunch+test.preparation.course+reading.score+writing.score , 
     data=p.student_train)

summary(reduced)

```
__There is a reduction in R-squared from 87% to 86%, thus model1 is preferred.__

__With the function prediction(), values for the math.score will be obtained,__
__against the observed math.score values.__
```{r}
y_pred <- predict(model1,newdata = p.student_train)
```
__Create a tibble with the predicted and the observed values.__
__Construct a plot y_pred mapped to the x position and the true y value__ __(p.student_train$math.score) mapped to the y value to examine the fit.__
__The line indicates that the fit is perfect.__
```{r}
tibble(pred = y_pred, 
       obs  = p.student_train$math.score) %>% 
  ggplot(aes(x = pred, y = obs)) +
  geom_point() +
  theme_minimal() +
  geom_abline(slope = 1)+
  labs(title = "Predicted versus observed Math Score")
```

__Residuals plot__
```{r}
plot(model1, pch=16, which=1)

```
__This residual plot is constructed from the multiple linear regression "model1".__
__The red line is a smooth fit to the residuals.__

__Also, from the histogram on the residuals we can see that they form__
__a normal distribution.__
```{r}

ggplot(data=p.student_train, aes(model1$residuals)) + 
    geom_histogram(binwidth = 1, color = "black", fill = "purple4")+
    theme(panel.background = element_rect(fill = "white"),
          axis.line.x=element_line(),
          axis.line.y=element_line()) +
    ggtitle("Histogram for TrainSet Model_1 Residuals") 
```


__Performing multiple regression to the test set for cross validation.__
```{r}
model2 <- lm(formula=math.score ~ . , data=p.student_test)
summary(model2)
```
__As it is possible to examine from the summary(), regression coefficient gender__
__and for male is associated with an increase of 13 points on math.score against females. __
__The regression coefficient Race ethnicity E has also significant association __
__with an increase of 4 points on math.score against Race ethnicity A.__


__However,regression coefficient race.ethnicities B,C and D do not reach statistical__
__significance. Same applies for the regression coefficient parental.level.of.education__
__for the education levels of: bachelor's degree,some college,master's degree __
__high school.__

__On the contrast regression coefficient parental.level.of.education __
__some high school is associated with a decrease of 2.4 points on math.score __
__against asscociate's degree.__

__The regression coefficient Lunch is associated with an increase of 2.8 points__
__on math.score against free/reduced.The regression coefficient Reading.score is__
__associated with an increase of 3.7 points on math.score.__

__The regression coefficient writing.score is associated__
__with an increas of 0.35 on math.score. The regression coefficient test.preparation__
__none is associated with an increase of 0.67 against completed preparation.__

```{r}
y_pred2 <- predict(model2,newdata = p.student_test)
```
__Create a tibble with the predicted and the observed values.__
__Construct a plot y_pred mapped to the x position and the true y value__ __(p.student_train$math.score) mapped to the y value to examine the fit.__
__The line indicates that the fit is perfect.__
```{r}
tibble(pred = y_pred2, 
       obs  = p.student_test$math.score) %>% 
  ggplot(aes(x = pred, y = obs)) +
  geom_point() +
  theme_minimal() +
  geom_abline(slope = 1)+
  labs(title = "Predicted versus observed Math Score")
```

```{r}
#95% confidence interval
prediction1 <- predict(model2, p.student_test, interval="confidence", level = 0.95)
```

__Residuals plot__
```{r}
plot(model2, pch=16, which=1)

```
__This residual plot is constructed from the multiple linear regression "model2".__
__The red line is a smooth fit to the residuals.__
__From the examination of the plot, the line is a smooth fit to the residuals.__
__Also, from the histogram on the residuals we can see that they form__
__normal distribution.__
```{r}

ggplot(data=p.student_test, aes(model2$residuals)) + 
    geom_histogram(binwidth = 1, color = "black", fill = "purple4")+
    theme(panel.background = element_rect(fill = "white"),
          axis.line.x=element_line(),
          axis.line.y=element_line()) +
    ggtitle("Histogram for TestSet Model_2 Residuals") 
```


__The training model and the test model have same R-squared value.__


